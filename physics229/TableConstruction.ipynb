{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Tables ##\n",
    "\n",
    "There is a surprising amount of interesting mathematics involved in the construction of numerical tables (e.g. trigonometric tables, or tables of square roots or logs).  Nowadays the tables themselves are generally uninteresting, but the mathematical history is fascinating and the techniques can be surprisingly relevant even with modern computational tools.  It's not infrequent that you want an approximation to some function (e.g. the mass enclosed within a specific radius, an integral of some physically interesting function, etc.) inside a loop, an ODE integration or to get started on some other problem and many of the methods below serve as standard workhorses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square root ##\n",
    "\n",
    "One of the most useful set of tables was a square root table, as taking square roots shows up in other places, like trig tables or log tables.  So we'll start with this.  One basic idea is to make a guess for the square root and then use [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method) iteration to improve it, i.e. to obtain $\\sqrt{a}$ we iterate\n",
    "$$\n",
    "  x_{n+1} = \\frac{1}{2}\\left( x_n + \\frac{a}{x_n}\\right)\n",
    "$$\n",
    "This converges quadratically near the solution.  Given a value for $\\sqrt{2}$ we can clearly reduce the range we need to actually solve to $[1/\\sqrt{2},\\sqrt{2}]$ by successive multiplications or divisions of $a$ by 2 (keeping track of the factors).\n",
    "\n",
    "Interestingly this algorithm was known to the Babylonians, and tables of square roots go back more than 3 millenia (check out [YBC7289](https://en.wikipedia.org/wiki/YBC_7289)).  Exactly how the square root was computed is not entirely clear, since doing long division base 60 with Babylonian methods is _complex_.  But any modern root finding method would work, so they may even have used bisection (which involves only averaging and squaring numbers, in this case, and hence is relatively easy).\n",
    "\n",
    "Suppose we've reduced our range to $a\\in[2^{-1/2},2^{1/2}]$.  We want to start with a close guess for $\\sqrt{a}$.  On a modern computer people actually use clever bit fiddling tricks, based on how IEEE floating point numbers are stored.  We can imagine a simple polynomial first guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how closely various polynomials approximate sqrt{a}.\n",
    "aa = np.linspace(1/np.sqrt(2.0),np.sqrt(2.0),50)\n",
    "for iorder in [1,2,3]:\n",
    "    pp = np.poly1d(np.polyfit(aa,np.sqrt(aa),iorder))\n",
    "    er = np.max(np.abs(pp(aa)-np.sqrt(aa))/np.sqrt(aa))\n",
    "    print(pp,\" has fractional error \",er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear that the increased accuracy of quadratic or cubic polynomials actually helps in this case, we may was well start with just linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_sqrt(a):\n",
    "    \"\"\"Implement a simple sqrt algorithm, assuming 1/sqrt{2}<a<sqrt{a}.\n",
    "       Returns both the 1-iteration and 2-iteration result.\"\"\"\n",
    "    xx = 0.5052 + 0.4899 * a\n",
    "    x1 = 0.5*(xx+a/xx)\n",
    "    x2 = 0.5*(x1+a/x1)\n",
    "    return( (x1,x2) )\n",
    "    #\n",
    "# Note how quickly Newton converges once you get close.  In this case we\n",
    "# need just one or two iterations.\n",
    "for a in np.linspace(0.71,1.4,10):\n",
    "    exact = np.sqrt(a)\n",
    "    x1,x2 = my_sqrt(a)\n",
    "    print(\"a={:f} has my_sqrt={:f} ({:f}% error) or {:f} ({:f}% error)\".\\\n",
    "          format(a,x1,100*(x1-exact)/exact,x2,100*(x2-exact)/exact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we could wrap this in a more general routine which did the reduction\n",
    "# in range as well.  We'd generally be more clever than the below, but this\n",
    "# illustrates the point.\n",
    "def my_sqrt(a):\n",
    "    \"\"\"Implements a square root algorithm, including range reduction.\"\"\"\n",
    "    sqrt2  = 1.4142135623730951  # Would need to be worked out in advance.\n",
    "    aa,fac = a,1.0\n",
    "    while aa>16:\n",
    "        aa,fac = aa/16.0,fac*4.0\n",
    "    while aa> 4:\n",
    "        aa,fac = aa/4.0,fac*2.0\n",
    "    while aa> 2:\n",
    "        aa,fac = aa/2.0,fac*sqrt2\n",
    "    while aa<1./16.:\n",
    "        aa,fac = aa*16,fac/4.0\n",
    "    while aa<1./4.:\n",
    "        aa,fac = aa*4.,fac/2.0\n",
    "    while aa<1./2.:\n",
    "        aa,fac = aa*2,fac/sqrt2\n",
    "    xx = 0.5052 + 0.4899 * aa\n",
    "    xx = 0.5*(xx+aa/xx)\n",
    "    xx = 0.5*(xx+aa/xx)\n",
    "    return(xx*fac)\n",
    "    #\n",
    "for a in np.logspace(-2.0,2.0,10):\n",
    "    ex = np.sqrt(a)\n",
    "    xx = my_sqrt(a)\n",
    "    print(\"a={:8.3f} has my_sqrt={:8.4f} ({:10.8f}% error)\".\\\n",
    "          format(a,xx,100*(xx-ex)/ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you do if you could only multiply, not divide?  This is still the case on some microcontrollers (though fewer and fewer nowadays). You could spend some time coming up with a Newton iteration for inverse, but that's Newton inside Newton and gets complicated.  The other thing is to think about $1/\\sqrt{a}$.  If I could find this, multiplication by $a$ gives $\\sqrt{a}$.  Note solving $f(x)=1/x^2 - a = 0$ using Newton's method gives the following iteration\n",
    "$$\n",
    "  x_{n+1} = x_n - \\frac{x_n^{-2}-a}{-2x_n^{-3}} = x_n + \\frac{1}{2}\\left(x_n-ax_n^2\\right) = \\frac{1}{2}x_n\\left( 3 - ax_n^2 \\right)\n",
    "$$\n",
    "which involves only multiplications.\n",
    "\n",
    "(As an aside, check out [this code](https://en.wikipedia.org/wiki/Fast_inverse_square_root) for a fast inverse square root with the highly amusing comments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trig tables ##\n",
    "\n",
    "Tables of sines, cosines, tangents, etc. have been in use for a long time, and clearly come into many practical calculations.  Early on (ancient Greek times) they were \"tables of chords\" of circles, but it's essentially the same thing.\n",
    "\n",
    "We could do this by Taylor series expansion (see below), but instead consider the following method.  Simple geometry is enough to tell us sine and cosine for a number of standard angles: $0^\\circ$, $30^\\circ$, $45^\\circ$, $60^\\circ$ and $90^\\circ$.  Clearly symmetry allows us to determine the functions for all other ranges once we know sine and cosine for $[0^\\circ,90^\\circ]$.\n",
    "\n",
    "We know the sine and cosine addition theorems, so for example:\n",
    "$$\n",
    "  \\sin(A+B) = \\sin A \\cos B + \\cos A \\sin B\n",
    "$$\n",
    "and this allows us to get sine and cosine for $15^\\circ$ and $75^\\circ$ easily.  At this point we could get sine and cosine for $7.5^\\circ$ using the double-angle formula and our results for $15^\\circ$.  But we know sine and cosine are smooth, so what if we just fit a polynomial to these results?  How well would we do?\n",
    "\n",
    "Back in the day, people would have used a [Newton divided difference polynomial](https://en.wikipedia.org/wiki/Newton_polynomial) for their interpolation.  Let's have NumPy do the work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we have the sine table at 15 degree steps.\n",
    "tt = np.array([0.0,15.,30.,45.,60.,75.,90.]) * np.pi/180.\n",
    "st = np.sin(tt)\n",
    "# Now fit a polynomial through this.  We have 7 points, so\n",
    "pp = np.poly1d( np.polyfit(tt,st,6) )\n",
    "print(pp)\n",
    "# How well does this do?\n",
    "tt = np.linspace(1e-4,np.pi/2.,200)\n",
    "er = (pp(tt)-np.sin(tt))/(np.sin(tt)+1e-8)\n",
    "# Let's plot the percentage error\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,6))\n",
    "ax.plot(tt*180./np.pi,100*er,'b-')\n",
    "ax.set_xlim([0,90])\n",
    "ax.set_xlabel(r'$\\theta$  [deg]',fontsize=18)\n",
    "ax.set_ylabel(r'Fractional error (percent)',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is good enough for about 4 digits of accuracy!  Not too shabby.  If we went down to 7.5 degrees we could improve this even more.  Also, we could probably use a power series for smaller values of $\\theta$ to improve accuracy to the left of the plot.  And we would probably interpolate across pieces of the table rather than finding a global polynomial fit.\n",
    "\n",
    "This kind of Newton-polynomial method was widely used.  Perhaps the coolest example was the Babbage difference engine, which used to be on display in the [Computer History Museum](http://www.computerhistory.org/) in Palo Alto, but is now only in the London science museum.  The principle of difference engines is [explained here](http://www.computerhistory.org/babbage/howitworks/).\n",
    "\n",
    "What if we fit a spline to the points, instead of a polynomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we have the sine table at 15 degree steps.\n",
    "tt = np.array([0.0,15.,30.,45.,60.,75.,90.]) * np.pi/180.\n",
    "st = np.sin(tt)\n",
    "# Now fit a Spline through this.\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline as Spline\n",
    "ss = Spline(tt,st)\n",
    "# How well does this do?\n",
    "tt = np.linspace(1e-5,np.pi/2.,200)\n",
    "er = (ss(tt)-np.sin(tt))/(np.sin(tt)+1e-8)\n",
    "# Let's plot the percentage error\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,6))\n",
    "ax.plot(tt*180./np.pi,100*er,'b-')\n",
    "ax.set_xlim([0,90])\n",
    "ax.set_xlabel(r'$\\theta$  [deg]',fontsize=18)\n",
    "ax.set_ylabel(r'Fractional error (percent)',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly the polynomial approximation does better than the spline!  Sometimes knowing the theory is better than \"machine learning\".\n",
    "\n",
    "What if we start to use some actual mathematical theory?  We can Taylor expand $\\sin x$ to any order we like.  What if we use an economized power series?  The basic idea is to start with our Taylor series\n",
    "$$\n",
    "  \\sin x = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots\n",
    "$$\n",
    "carried out to very high order.  Map the $x$-range of interest into $-1\\le y\\le 1$.  Convert the powers of $y$ into [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials) ($T_n$), truncate the expansion and then convert back to powers of $y$ and hence $x$.  (This is actually how people first coded these things up back in the good old days of digital computers.)\n",
    "\n",
    "The remapping step can be implemented by looking at $\\sin(\\pi y/2)$ over the range $-1\\le y\\le 1$. So\n",
    "$$\n",
    "  \\sin\\frac{\\pi y}{2} \\approx \\frac{\\pi}{2}y - \\frac{\\pi^3y^3}{48} + \\frac{\\pi^5y^5}{3840}\n",
    "  = 1.13613 T_1 - 0.136587 T_3 + 0.00498079 T_5\n",
    "$$\n",
    "If we truncate at $T_3$ (i.e. throw away $T_5$) we get\n",
    "$$\n",
    "  \\sin\\frac{\\pi y}{2}\\approx  1.13613 T_1 - 0.136587 T_3\n",
    "  = 1.54589 y - 0.546348 y^3 = 0.984146 x - 0.140965 x^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well does this work?\n",
    "pp = np.poly1d([-0.140965,0.0,0.984146,0.0])\n",
    "xx = np.linspace(0.,np.pi/2.,100)\n",
    "ex = np.sin(xx)\n",
    "me = pp(xx)\n",
    "print(\"Maximum error, {:f}%\".format(100*np.max(np.abs(me-ex)/(me+1e-10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good for a 2 term approximation!  Note with a little cleverness this requires only a very few multiplications and additions.\n",
    "\n",
    "What if we go to $x^7$?  The series becomes\n",
    "$$\n",
    "  \\sin x \\approx 0.999996 x - 0.166647 x^3 + 0.00830502 x^5 - 0.000183114 x^7\n",
    "$$\n",
    "How well does that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.poly1d([-0.000183114,0,0.00830502,0,-0.166647,0,0.999996,0])\n",
    "xx = np.linspace(0.,np.pi/2.,100)\n",
    "ex = np.sin(xx)\n",
    "me = pp(xx)\n",
    "print(\"Maximum error, {:f}%\".format(100*np.max(np.abs(me-ex)/(me+1e-10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's much better than our $15^\\circ$ degree polynomial interpolation, but it would be more work to evaluate than interpolating from a table of numbers.  But it could be used to construct such a table!  More interestingly, this is how standard math libraries (or hardware on high-end chips) do trig functions.  Note if we call the coefficients in the $\\sin x$ expansion above $c_n$ we could write\n",
    "$$\n",
    "  \\sin x \\approx x*(c_0+x^2*(c_3+x^2*(c_5+x^2*c_7)))\n",
    "$$\n",
    "which involves 4 multiplications and 3 additions!  Compare the coefficients with the source code [here](http://www.netlib.org/fdlibm/k_sin.c), which goes to 13th order.\n",
    "\n",
    "Another thing about Chebyshev expansions is that if you're fitting tabulated data with a series of polynomials, using orthogonal polynomials (of which Chebyshev is an example) then the condition number of the matrix you need to invert is generally **much** lower than if you use monomials like $x^n$.\n",
    "\n",
    "Not directly relevant to us here, but if you're curious check out the [CORDIC](https://en.wikipedia.org/wiki/CORDIC) method for solving sine and cosine functions using the fact that computers are very good at dividing by 2.\n",
    "\n",
    "Also, if you like to read about how sometimes the simplest steps cause problems, take a look at the problems with the built-in sine routine on Intel processors at [Intel Underestimates Error Bounds by 1.3 quintillion](https://randomascii.wordpress.com/2014/10/09/intel-underestimates-error-bounds-by-1-3-quintillion/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Chebyshev economization ##\n",
    "\n",
    "One can do the economization of power series entirely automatically with standard numerical routines, but just for fun here's a method using NumPy's polynomial class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a table of Chebyshev polynomials.\n",
    "Tn = [ np.poly1d([1.0]) , np.poly1d([1.0,0.0]) ]\n",
    "twox = np.poly1d([2.0,0.0])\n",
    "for n in range(2,11):\n",
    "    Tn.append( twox*Tn[-1] - Tn[-2] )\n",
    "# Print the first few Chebyshev polynomials, just for fun.\n",
    "for pp in Tn[:4]:\n",
    "    print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now set up a list of coefficients of the first term.\n",
    "leading = [1.] + [2.**i for i in range(10)]\n",
    "# Make up a series to approximate.  Here we'll do cosine.\n",
    "coeff = [1.0,0.0,-1./2.,0.0,1./24.,0.0,-1./720.,0.0,1./40320.,0.0,-1./3628800.]\n",
    "# We want this to be cos(pi x/2) to do the range reduction:\n",
    "coeff = np.array(coeff)*np.array([(np.pi/2)**i for i in range(len(coeff))])\n",
    "# Turn this into a polynomial.\n",
    "cosx = np.poly1d(coeff[::-1])\n",
    "print(cosx)\n",
    "# Now we economize it.\n",
    "# Subtract off the Chebyshev polynomials for the highest terms, here we do 2.\n",
    "for i in range(2):\n",
    "    degree  = len(cosx.c)-1\n",
    "    cosx   -= cosx.c[0]/leading[degree] * Tn[degree]\n",
    "    print(cosx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can test it.\n",
    "xx = np.linspace(0.,1.,100)\n",
    "ex = np.cos(xx*np.pi/2)\n",
    "me = cosx(xx)\n",
    "print(\"Maximum error, {:f}%\".format(100*np.max(np.abs(me-ex)/(me+1e-10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithm ##\n",
    "\n",
    "The final set of tables we'll consider here are log tables.  This is included primarily because the method first developed is so slick it just feels like the kind of thing every geek should know.  Back in 1624 [Henry Briggs](https://en.wikipedia.org/wiki/Henry_Briggs_(mathematician)) published his \"Arithmetica Logarithmica\", containing the logs of thirty thousand natural numbers to fourteen decimal places.  This was worked out with quill pen!  How did he do it?\n",
    "\n",
    "Some are easy.  If I knew $\\ln 2$ and $\\ln 3$ I would know $\\ln 6$, $\\ln 12$, etc.  So I'm really only interested in the primes.  We know how to compute logs for numbers close to 1 by Taylor expansion.  If we know some logs, e.g. $\\ln a$, we can use $\\ln(ax)=\\ln a+\\ln x$ to reduce the size of the argument.  We also know that $\\ln\\sqrt{x}=(1/2)\\ln x$, and $\\sqrt{x}$ is closer to $1$ than $x$ is.  So you first divide out by factors you know, then you take square roots until $x$ is close to $1$, then you use an approximation for $\\ln(1+x)$.  And you do this 30,000 times!\n",
    "\n",
    "For numbers close to 1 the Taylor expansion is:\n",
    "$$\n",
    "  \\ln(1+x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\cdots\n",
    "$$\n",
    "This series isn't as quickly converging as one might like, but it's okay for very small $x$.  One could \"economize\" this series too, but actually slightly slicker would be to do a [Pade approximant](https://en.wikipedia.org/wiki/Pad%C3%A9_approximant).  This is a rational function whose Taylor series agrees with the above up to a given order.  This requires slightly fewer multiplications and divisions.\n",
    "\n",
    "You can quickly convince yourself that\n",
    "$$\n",
    "  \\ln(1+x) \\simeq \\frac{x + x^2/2}{1+x+x^2/6}\n",
    "$$\n",
    "agrees with the Taylor series for $\\ln(1+x)$ up to and including the $x^4$ term but only really requires us to square $x$ (and some additions and divisions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How close is our (2,2) Pade approximant to Log[1+x] for 0<x<sqrt{2}-1?\n",
    "xx = np.linspace(0.1,np.sqrt(2.0)-1.0,200)\n",
    "ex = np.log(1+xx)\n",
    "pa = (xx+xx**2/2)/(1+xx+xx**2/6)\n",
    "print(\"Worst case error is {:e} percent.\".format(100*np.max(np.abs(pa-ex)/ex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can compute $ln x$ very accurately for $1<x<\\sqrt{2}$.  Take out any factors you know the logs of if you want, but it's not really necessary.  If $x<1$ we use $\\ln x^{-1}=-\\ln x$.  We then take square roots until $x$ gets less than $\\sqrt{2}$ and use our approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_log(x):\n",
    "    \"\"\"Computes ln(x) using Brigg's method.\"\"\"\n",
    "    if x<1:\n",
    "        return(-my_log(1./x))\n",
    "    xx,fac = x,1.0\n",
    "    while xx>np.sqrt(2.0):\n",
    "        xx,fac = np.sqrt(xx),2*fac\n",
    "    xx -= 1.0\n",
    "    lnx = (xx+xx**2/2)/(1+xx+xx**2/6)\n",
    "    return( fac*lnx )\n",
    "    #\n",
    "for x in np.arange(2,21):\n",
    "    ex = np.log(x)\n",
    "    me = my_log(x)\n",
    "    er = (me-ex)/ex\n",
    "    print(\"Ln({:4.1f})={:f} ({:f}% error)\".format(x,me,100*er))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not quite up to Brigg's standards, but it's pretty good already!\n",
    "\n",
    "There's a [funny story from the early days of electronic calculators](http://www.hpmemoryproject.org/an/pdf/A_Quarter_Century_at_HP110829.pdf) when people were first implementing these sorts of methods and CORDIC in particular:\n",
    "\n",
    "\"After the introduction of the 9100 our legal department got a letter from Wang saying that we had infringed on their patent. And I just sent a note back with the Briggs reference in Latin and it said, \"It looks like prior art to me.\" We never heard another word.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rational function expansion. ##\n",
    "\n",
    "As a further example of the power of rational function expansion and Pade approximants, let's consider computing $\\arctan x$.  By power series\n",
    "$$\n",
    "  \\arctan x \\approx x - \\frac{1}{3}x^3 + \\frac{1}{5}x^5 - \\frac{1}{7}x^7 + \\frac{1}{9}x^9 - \\cdots\n",
    "$$\n",
    "You can match this power series with a ratio of polynomials as e.g.\n",
    "$$\n",
    "  \\arctan x \\approx \\frac{x+(7/9)x^3+(64/945)x^5}{1+(10/9)x^2 + (5/21)x^4}\n",
    "$$\n",
    "How well do these two compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taylor = np.poly1d([1./9.,0,-1./7.,0,1./5.,0,-1./3.,0,1,0])  # Go through x^9 inclusive.\n",
    "pnum   = np.poly1d([64./945.,0,7./9.,0,1,0])  # Highest power is x^5.\n",
    "pden   = np.poly1d([5./21.,0,10./9.,0,1])     # Highest power is x^4.\n",
    "#\n",
    "for xx in np.linspace(0.5,1,6,endpoint=True):\n",
    "    tapp = taylor(xx)\n",
    "    papp = pnum(xx)/pden(xx)\n",
    "    exact= np.arctan(xx)\n",
    "    print(\"x={:6.2f}, Taylor={:8.5f} ({:12.4e}%), Pade={:8.5f} ({:12.4e}%)\".\\\n",
    "          format(xx,tapp,100*(tapp-exact)/exact,papp,100*(papp-exact)/exact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And what happens if we keep going?\n",
    "for xx in np.arange(1.0,2.51,0.25):\n",
    "    tapp = taylor(xx)\n",
    "    papp = pnum(xx)/pden(xx)\n",
    "    exact= np.arctan(xx)\n",
    "    print(\"x={:6.2f}, Taylor={:8.4f} ({:12.4e}%), Pade={:8.5f} ({:12.4e}%)\".\\\n",
    "          format(xx,tapp,100*(tapp-exact)/exact,papp,100*(papp-exact)/exact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the catastrophic behavior of the power series expanded beyond its range of validity.  By contrast the Pade approximant is behaving quite well.  It's losing accuracy but not as catastrophically.  This behavior isn't guaranteed, but is typical.\n",
    "\n",
    "There is a lot of interesting theory behind _why_ Pade approximants can do so well, involving complex analysis.  You can find this discussed in most books on numerical analysis (and whole courses in math departments!).\n",
    "\n",
    "It should come as no surprise that you can combine Chebyshev expansions and rational function expansions together to approximate functions as ratios of series of Chebyshev polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rational function approximation ##\n",
    "\n",
    "The theory of rational function approximation can quickly become quite complex, and there isn't an algorithm as simple as there is for Chebyshev economization.  But we're not usually in the situation where we're designing microcode for a mass-produced computer chip and billions of dollars rests on the optimality of our results.  Normally we have a function we want to approximate efficiently (e.g. in the inner loop of some ODE integration or something we need to apply to a huge data set) but we don't want to devote a year to the effort and can afford some slight imperfection.  In this case the following algorithm works \"just fine\".\n",
    "\n",
    "We have a function, $f(x_i)$, that we can evaluate at predetermined points $\\{x_i\\}$.  Usually choosing equally spaced points is a bad idea (as it is for polynomial interpolation due to Runge's phenomenon), but there's no hard and fast rule.  Often roots of a Chebyshev polynomial work well.  We approximate this as\n",
    "$$\n",
    "  f(x) \\approx \\frac{\\sum_{n=0}^{A} a_n x^n}{1+\\sum_{n=1}^B b_n x^n}\n",
    "  \\quad \\Rightarrow \\quad\n",
    "  f(x) + \\sum_{n=1}^B b_n f(x)x^n = \\sum_{n=0}^{A} a_n x^n\n",
    "$$\n",
    "How do we choose the $a_n$ and $b_n$?  Again, there's a \"right\" way which is pretty complicated but there's an easy way which is ... easy.  Let's minimize\n",
    "$$\n",
    "  \\chi^2 = \\sum_i \\left[ f(x_i) + \\sum_{n=1}^B b_n f(x_i)x_i^n - \\sum_{n=0}^{A} a_n x_i^n \\right]^2\n",
    "$$\n",
    "To make this easier to manipulate, define $c_\\alpha=\\{a_n,b_n\\}$, i.e. to be $a_n$ for $n=0$ to $A$ and then $b_n$ for $n=1$ to B.  Similarly define $\\phi_{i\\alpha}$ to be $x_i^n$ for $0\\le\\alpha\\le n$ and $-f(x_i)x_i^n$ for the remaining $B-1$ elements. This enables us to write\n",
    "$$\n",
    "  \\chi^2 = \\sum_i \\left[ f(x_i) - \\sum_{\\alpha} c_\\alpha \\phi_{i\\alpha} \\right]^2\n",
    "  \\quad \\Rightarrow \\quad\n",
    "  \\frac{d\\chi^2}{dc_\\alpha} = \\sum_i\\left[ f(x_i) - \\sum_{\\beta} c_\\beta \\phi_{i\\beta} \\right] (-2\\phi_{i\\alpha})\n",
    "$$\n",
    "and $\\chi^2$ is minimized when\n",
    "$$\n",
    "  c_\\alpha = \\left[ \\sum_i \\phi_{i\\alpha}\\phi_{i\\beta}\\right]^{-1} \\sum_j f(x_j)\\phi_{j\\beta}\n",
    "$$\n",
    "Providing we aren't talking hundreds of terms, this is an almost trivial problem (note we can have an arbitrary number of $x_i$, the difficulty of the matrix inversion depends only on $A+B$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example ###\n",
    "\n",
    "Let's see how the code does at guessing $\\arctan x$ if we don't tell it the power series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to find the coefficiencts of a rational approximation to a tabulated function.\n",
    "def find_rational(xi,fxi,A,B):\n",
    "    \"\"\"Finds the coefficients of the rational approximation to f(x) [see above].\"\"\"\n",
    "    # First set up phi_{j\\alpha}.  Can do this with cleverness, but let's\n",
    "    # try to make the code more readable -- runtime isn't an issue.\n",
    "    Ndata = len(xi)\n",
    "    Nbase = A+B+1\n",
    "    phi   = np.zeros( (Ndata,Nbase) )\n",
    "    for i in range(Ndata):\n",
    "        for a in range(Nbase):\n",
    "            phi[i,a] = xi[i]**a if a<=A else (-fxi[i]*xi[i]**(a-A))\n",
    "    # Now set up the \"source\" and solve for c.\n",
    "    d = np.dot( fxi,phi )\n",
    "    M = np.dot( phi.T,phi )\n",
    "    c = np.dot( np.linalg.inv(M),d )\n",
    "    num=np.poly1d(c[:A+1][::-1])\n",
    "    den=np.poly1d(np.append(c[A+1:][::-1],1.0))\n",
    "    return( (num,den) )\n",
    "    #\n",
    "# Choose points at random rather than trying something sophisticated.\n",
    "xx = np.sort(np.random.uniform(size=100))\n",
    "fx = np.arctan(xx)\n",
    "# Let's first look for a low-order approximation -- this gets harder\n",
    "# and harder as the order increases and you often want some sort of\n",
    "# regularization or cleverness on the fit.\n",
    "A,B     = 3,2\n",
    "num,den = find_rational(xx,fx,A,B)\n",
    "print(num)\n",
    "print(den)\n",
    "print(\"Maximum error {:10.4f}%\".format(100*np.max(np.abs(fx-num(xx)/den(xx))/fx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And plot the results.\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "ax.plot(xx,fx,label='f(x)')\n",
    "ax.plot(xx,num(xx)/den(xx),label='Rational')\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$x$',fontsize=18)\n",
    "ax.set_ylabel(r'$f(x)$',fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try higher order -- note putting in the odd symmetry explicitly\n",
    "# by fitting over the range [-1,1] helps both the fit and the\n",
    "# numerical stability of the matrix.  Look at the size of the\n",
    "# even vs. odd powers of x below.\n",
    "xx = np.linspace(-1,1,100)\n",
    "fx = np.arctan(xx)\n",
    "#\n",
    "A,B     = 5,4\n",
    "num,den = find_rational(xx,fx,A,B)\n",
    "print(num)\n",
    "print(den)\n",
    "print(\"Maximum error {:10.4f}%\".format(100*np.max(np.abs(fx-num(xx)/den(xx))/fx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High precision ##\n",
    "\n",
    "And one last thing: what if we wanted to do really high precision?  Rather than get into too many details, I highlight one super-cool connection between high precision elementary functions and the theory of elliptic integrals.  For more information take a look at [Borwein & Borwein  \"The arithmetic-geometric mean and fast computation of elementary functions\"; SIAM review 26, 351 (1984)](https://epubs.siam.org/doi/10.1137/1026073).\n",
    "\n",
    "Define the \"arithmetic-geometric mean\" (AGM) of two positive numbers ($a>0$ and $b>0$) as the following iteration: starting from $a_0=a$ and $b_0=b$ iterate\n",
    "$$\n",
    "  a_{n+1} = \\frac{1}{2}\\left( a_n+b_n \\right) \\quad , \\quad\n",
    "  b_{n+1} = \\sqrt{a_nb_n}\n",
    "$$\n",
    "It takes only a little work to show $a_n$ and $b_n$ converge to a common limit, and in fact the convergence is very fast.  Specifically $c_n=\\sqrt{a_n^2-b_n^2}$ converges quadratically to zero.\n",
    "\n",
    "Gauss was the first to provide a way to compute elementary functions using algorithms that converged extremely rapidly (Newton's method already does this for algebraic functions).  He did this by showing\n",
    "$$\n",
    "  I(a,b) = \\frac{\\pi}{2}{\\rm AGM}(a,b) = \\int_0^{\\pi/2}\\frac{d\\theta}{\\sqrt{a^2\\cos^2\\theta+b^2\\sin^2\\theta}}\n",
    "$$\n",
    "The integral is an example of a _complete elliptic integral of the first kind_.  It satisfies many interesting identities, for example\n",
    "$$\n",
    "  \\lim_{k\\to 0^{+}}\\left[\\ln\\left(\\frac{4}{k}\\right)-I(1,k)\\right] = 0\n",
    "$$\n",
    "With a little rearranging and some attention to limits you can show\n",
    "$$\n",
    "  \\left| \\ln(x) - \\left[ I(1,10^{-n})-I(1,10^{-n}x)\\right]\\right| < n 10^{-2(n-1)}\n",
    "$$\n",
    "for $0<x<1$ and $n\\ge 3$.  Thus we can compute $\\ln x$ exponentially quickly given the AGM method for computing $I(a,b)$.  Similar tricks give $\\exp(x)$ and hence $\\sin x$, $\\cos x$, $\\sinh x$, $\\cosh x$, etc.\n",
    "\n",
    "We can also compute constants.\n",
    "\n",
    "The method for $\\pi$ is:\n",
    "1. Set $\\alpha_0=\\sqrt{2}$, $\\beta_0=0$ and $\\pi_0=2+\\sqrt{2}$.\n",
    "2. Iterate\n",
    "   * $\\alpha_{n+1} = \\frac{1}{2}\\left(\\alpha_n^{1/2}+\\alpha_n^{-1/2}\\right)$\n",
    "   * $\\beta_{n+1} = \\alpha_n^{1/2}\\left(\\frac{\\beta_n+1}{\\beta_n+\\alpha_n}\\right)$\n",
    "   * $\\pi_{n+1} = \\pi_n\\beta_{n+1}\\left(\\frac{1+\\alpha_{n+1}}{1+\\beta_{n+1}}\\right)$\n",
    "\n",
    "with $\\pi_n\\to\\pi$ as $n\\to\\infty$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Pi to a ridiculous number of digits.\n",
    "from mpmath import mp\n",
    "mp.dps = 50\n",
    "#\n",
    "one   = mp.mpf(1.0)\n",
    "half  = mp.mpf(0.5)\n",
    "alpha = mp.sqrt(mp.mpf(2.0))\n",
    "beta  = mp.mpf(0.0)\n",
    "pi    = mp.mpf(2.0) + alpha\n",
    "#\n",
    "for iter in range(6):\n",
    "    aa    = alpha\n",
    "    rtalp = mp.sqrt(alpha)\n",
    "    alpha = half*(rtalp+one/rtalp)\n",
    "    beta  = rtalp*(one+beta)/(aa+beta)\n",
    "    pi    = pi*beta*(one+alpha)/(one+beta)\n",
    "    print(pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
