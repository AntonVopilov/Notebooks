{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from an NFW profile ##\n",
    "\n",
    "Dark matter halos tend to have a characteristic shape, or density profile.  There are several fitting functions available, and much ink has been spilled about the relative merits of each.  The most commonly adopted is a double-power-law form, known as the [NFW profile](https://en.wikipedia.org/wiki/Navarro%E2%80%93Frenk%E2%80%93White_profile) after the initials of its authors:\n",
    "$$\n",
    "  \\rho(r) \\propto x^{-1}(1+x)^{-2}\\qquad x\\equiv \\frac{r}{r_s}\n",
    "$$\n",
    "where $x$ is a dimensionless radius and $r_s$ is the scale radius of the halo defined by the point where the log-slope is $-2$.  This form has two free parameters: the normalization and $r_s$.  However traditionally we use the total mass, $M_{\\rm vir}$, and concentration, $c=r_{\\rm vir}/r_s$, instead.  Since the profile doesn't integrate to finite mass the choice of $M_{\\rm vir}$ is conventional.  While many choices are possible, a sensible choice (motivated by spherical top-hat collapse) is the mass within which the mean density is $200\\times$ the background density of the Universe.  It then follows by definition that $M_{\\rm vir} = (800\\pi/3)\\bar{\\rho}_m\\,r_{\\rm vir}^3$.  An alternative is to use the critical density instead of the mean density to define $r_{\\rm vir}$.  Other conventions are also adopted.  While a full understanding of these universal profiles still eludes us, many\n",
    "aspects of halo formation may be understood from fairly general considerations of gravitational collapse.\n",
    "\n",
    "It is cosmologically interesting that $M$ and $c$ tend to be correlated but for our purposes now we'll just assume $M$ and $c$ are known.  Physically, the inner, $r^{−1}$, part of the halo forms early, and then $r_s$ stays pretty constant as subsequently accreted dark matter is mostly kept away from the center by the angular momentum barrier. Thus the halo concentration, $c\\equiv r_{\\rm vir}/r_s$, grows with time; simulations show that $c$ typically grows (roughly) linearly with scale factor: $a = (1+z)^{−1}$.  The average mass accretion history of halos is (approximately) exponential in redshift, $z$, and the angular momentum parameter, $\\lambda$, of the halo typically grows significantly in halo major mergers (i.e., mergers with mass ratios between unity and 1/3) and declines as mass is accreted in minor mergers. Dark matter halos are generally triaxial spheroids; they are more elongated at smaller radii, larger redshifts, and higher masses (perhaps reflecting early accretion from narrow filaments, with accretion becoming more spherical as the filaments grow thicker than the halos).\n",
    "\n",
    "It is frequently the case that we want to randomly draw radii (e.g. of satellite galaxies or dark matter particles) from the NFW distribution.  How would we go about doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of sampling from a known probability distribution, a problem that comes up frequently.\n",
    "\n",
    "NumPy already has a lot of distributions built in:\n",
    "https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html\n",
    "and SciPy takes this to the next level\n",
    "https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "But what if your distribution is not in the list (e.g. our case)?  You have three basic options\n",
    "1. [Rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling).\n",
    "2. Do CDF inverse analytically\n",
    "3. Do CDF inverse numerically/approximately\n",
    "\n",
    "The first is often the quickest to get going, though not always the most numerically efficient.  \n",
    "\n",
    "The other two are built upon the realization that for any $p(x)$ the cumulative distribution has a uniform distribution.  That means if I define $dy=p(x)\\,dx$ then draw $y$ from a uniform distribution the implicitly defined $x(y)$ will have distribution $p(x)$.  If you can explicitly do the inverse function $x(y)$ then you should, and this method will be the best you can do.  If the NFW profile were a single power-law, then we would be in this situation.  But it's not.\n",
    "\n",
    "Let's start with rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfw(c):\n",
    "    \"\"\"Returns a single draw from an NFW of concentration 'c' using rejection sampling.\"\"\"\n",
    "    while True:\n",
    "        x = np.random.uniform(size=1) * c\n",
    "        r = np.random.uniform(size=1)\n",
    "        # Note 4x/(1+x)^2 is x^2\\rho normalized to unity at peak.\n",
    "        if r< 4*x/(1+x)**2:\n",
    "            return(x[0]/c)\n",
    "    #\n",
    "# Now time how long this takes.  Rejection sampling is\n",
    "# fast in languages such as C/C++ or Fortran but tends\n",
    "# to be slow in Python.  If this is not an issue, this\n",
    "# is often the quickest-to-solution method.\n",
    "%time nfw(7.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be the algorithm under the hood of the SciPy [rvs_ratio_uniforms](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rvs_ratio_uniforms.html#scipy.stats.rvs_ratio_uniforms) method, but it's not stated explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution -- obviously not how we'd do many\n",
    "# draws in real life, but good enough for now ...\n",
    "cc  = 7.0\n",
    "%time rad = np.array([nfw(cc) for i in range(5000)])\n",
    "# Compare the histogram to r^2\\rho(r).\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.hist(rad,bins=20)\n",
    "r   = np.linspace(1e-2,1,100)\n",
    "x   = r*cc\n",
    "rho = 1400 * x**2 / x / (1+x)**2\n",
    "ax.plot(r,rho)\n",
    "ax.set_xlabel(r'$r/r_{\\rm vir}$',fontsize=18)\n",
    "ax.set_ylabel(r'$r^2\\ \\rho(r)$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that worked and was pretty quick to code!  For this level of use waiting <1 sec is clearly perfectly okay, but it might not scale to millions of draws.\n",
    "\n",
    "What about the other two methods?  Method (2) won't work for us.  It's actually quite easy to look at the mass enclosed within radius $x$ for an NFW profile (i.e. the CDF) since the integrals are elementary:\n",
    "$$\n",
    "  M(<x) \\propto \\ln(1+x)-\\frac{x}{1+x}\n",
    "$$\n",
    "the difficulty lies in inverting this to get $x(M)$.  Sometimes the [Lagrange inversion formula](https://en.wikipedia.org/wiki/Lagrange_inversion_theorem) is helpful in conjunction with other tricks for power series manipulation.  In our particular case this won't work due to the structure of the NFW profile.\n",
    "\n",
    "The simplest way around this is to built a table of the CDF then just interpolate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 100 # Number of table entries.\n",
    "cc = 7\n",
    "xx = np.linspace(0,cc,Nt)\n",
    "rr = xx/cc  # r/r_vir.\n",
    "Mx = np.log(1+xx)-xx/(1+xx)\n",
    "Mx/= Mx[-1]\n",
    "#\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].plot(rr,Mx)\n",
    "ax[0].set_xlabel(r'$r/r_{\\rm vir}$',fontsize=18)\n",
    "ax[0].set_ylabel(r'$M/M_{\\rm vir}$',fontsize=18)\n",
    "ax[1].plot(Mx,rr)\n",
    "ax[1].set_xlabel(r'$M/M_{\\rm vir}$',fontsize=18)\n",
    "ax[1].set_ylabel(r'$r/r_{\\rm vir}$',fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can sample from this -- note the inversion of Mx and rr order.\n",
    "xx  = np.random.uniform(size=5000)\n",
    "%time rad = np.interp(xx,Mx,rr)\n",
    "# Compare the histogram to r^2\\rho(r).\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.hist(rad,bins=20)\n",
    "r   = np.linspace(1e-2,1,100)\n",
    "x   = r*cc\n",
    "rho = 1400 * x**2 / x / (1+x)**2\n",
    "ax.plot(r,rho)\n",
    "ax.set_xlabel(r'$r/r_{\\rm vir}$',fontsize=18)\n",
    "ax.set_ylabel(r'$r^2\\ \\rho(r)$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that also worked well, and was almost two orders of magnitude faster because we used the library function np.interp to do the hard work (this is coded in C).  To be fair we probably should have compared this to a C coded version of rejection sampling -- but then wrapping that probably defeats the main advantage of rejection sampling: quick time-to-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to find the coefficiencts of a rational approximation to a tabulated function.\n",
    "def find_rational(xi,fxi,A,B):\n",
    "    \"\"\"Finds the coefficients of the rational approximation to f(x) [see above].\"\"\"\n",
    "    # First set up phi_{j\\alpha}.  Can do this with cleverness, but let's\n",
    "    # try to make the code more readable -- runtime isn't an issue.\n",
    "    Ndata = len(xi)\n",
    "    Nbase = A+B+1\n",
    "    phi   = np.zeros( (Ndata,Nbase) )\n",
    "    for i in range(Ndata):\n",
    "        for a in range(Nbase):\n",
    "            phi[i,a] = xi[i]**a if a<=A else (-fxi[i]*xi[i]**(a-A))\n",
    "    # Now set up the \"source\" and solve for c.\n",
    "    d = np.dot( fxi,phi )\n",
    "    M = np.dot( phi.T,phi )\n",
    "    c = np.dot( np.linalg.inv(M),d )\n",
    "    num=np.poly1d(c[:A+1][::-1])\n",
    "    den=np.poly1d(np.append(c[A+1:][::-1],1.0))\n",
    "    return( (num,den) )\n",
    "    #\n",
    "num,den = find_rational(Mx,rr,3,3)\n",
    "print(num)\n",
    "print(den)\n",
    "# Find the maximum error.\n",
    "mm = np.linspace(0.05,1,500,endpoint=True)\n",
    "ex = np.interp(mm,Mx,rr)\n",
    "pa = num(mm)/den(mm)\n",
    "err= np.abs((pa-ex)/ex)\n",
    "print(\"Maximum error {:f}% at M={:f}\".format(100*np.max(err),mm[np.argmax(err)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two graphically, just because ...\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "ax.plot(Mx,rr,label='Table')\n",
    "mm = np.linspace(0,1,50,endpoint=True)\n",
    "ax.plot(mm,num(mm)/den(mm),label='Rational')\n",
    "ax.set_xlabel(r'$M/M_{\\rm vir}$',fontsize=18)\n",
    "ax.set_ylabel(r'$r/r_{\\rm vir}$',fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And look at our usual distribution.\n",
    "xx  = np.random.uniform(size=5000)\n",
    "%time rad = num(xx)/den(xx)\n",
    "# Compare the histogram to r^2\\rho(r).\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.hist(rad,bins=20)\n",
    "r   = np.linspace(1e-2,1,100)\n",
    "x   = r*cc\n",
    "rho = 1400 * x**2 / x / (1+x)**2\n",
    "ax.plot(r,rho)\n",
    "ax.set_xlabel(r'$r/r_{\\rm vir}$',fontsize=18)\n",
    "ax.set_ylabel(r'$r^2\\ \\rho(r)$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this was actually even faster, and not very much more work!  The result would be a very portable piece of code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
