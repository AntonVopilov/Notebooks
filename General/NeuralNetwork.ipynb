{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ##\n",
    "\n",
    "A neural network is a complex, non-linear function with a large number of free parameters than can be tuned to make the network act as a \"universal function interpolator\".  In takes a vector of inputs, $\\vec{x}$, and produces a vector of outputs, $\\vec{y}$.\n",
    "\n",
    "In the simplest form of network there is an input layer, and output layer and one \"hidden\" layer.  Each layer is an array of (artificial) \"neurons\" which are simply functions which take inputs and compute a non-linear function of the inputs to produce a single output.  In the prototypical example a neuron would have weights $w_i$ and compute\n",
    "$$\n",
    "  \\mathrm{Output} = \\tanh\\left( \\sum_{i=1}^N w_i \\mathrm{Input}_i + w_0 \\right)\n",
    "$$\n",
    "where $\\tanh$ is the non-linear \"activation function\" and $w_0$ is a \"bias\" which can be adjusted along with the \"weights\" $w_i$.  The outputs of these neurons then feed into other neurons and so on. (People don't actually use a tanh, but rather the closely related \"sigmoid\" function or some other non-linear function but a tanh works fine for the purposes of this notebook.)\n",
    "\n",
    "The network is \"trained\" by taking a set of known inputs and outputs and minimizing a loss function (or maximizing a score) which is typically something like\n",
    "$$\n",
    "  \\mathrm{Loss} = \\sum \\left[ \\vec{y}_a - \\vec{O}(\\vec{x}_a)\\right]^2\n",
    "$$\n",
    "where $\\vec{O}(\\vec{x})$ is the output of the neural network.  The standard way to find the weights ($w_i$ for each neuron) which minimize the loss function is to use gradient descent. Computing the gradient of the loss function is just a long exercise in the chain rule -- which becomes a set of matrix multiplications.  At each step rather than apply a full gradient step one usually moves only part way along the gradient.  The scaling factor is known as the \"learning rate\".  Ideally one would use something like Newton's method, so that the scaling was estimated from the Hessian matrix.  But this is typically not done because of the complexity of computing the Hessian.  Instead various heuristics are often employed (going by names like Nesterov acceleration, AdaProp, ADAM, ...).\n",
    "\n",
    "It's probably easiest to just look at the code below to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A Python class to implement a very basic neural network with one\n",
    "    hidden layer.  The network uses a tanh activation and is trained\n",
    "    using back-propagation.\n",
    "    Example usage for an XOR function:\n",
    "      nn = NeuralNetwork(2,2,1)\n",
    "      tset = [ [[0,0],[0]] , [[0,1],[1]], [[1,0],[1]], [[1,1],[0]] ]\n",
    "      nn.train(tset)\n",
    "      nn.test(tset)\n",
    "    \"\"\"\n",
    "    __author__ = \"Martin White\"\n",
    "    __version__ = \"1.0\"\n",
    "    __email__  = \"mwhite@berkeley.edu\"\n",
    "\n",
    "    def interp(self,inputs):\n",
    "        \"\"\"Evaluate the neural network at the inputs.\n",
    "        The inputs are provided unscaled, they are scaled internally.\"\"\"\n",
    "        if len(inputs)!= self.ni-1:\n",
    "            raise ValueError(\"Inputs wrong length in update.\")\n",
    "        self.ai[:-1] = (np.array(inputs)-self.xmin)/(self.xmax-self.xmin)\n",
    "        self.ah = np.tanh( np.dot(self.ai,self.wi) )\n",
    "        self.ao = np.tanh( np.dot(self.ah,self.wo) )\n",
    "        return((self.zmax-self.zmin)*self.ao[:]+self.zmin)\n",
    "\n",
    "    def back_propagate(self,targets,eps,ups):\n",
    "        \"\"\"Does a single backwards propagation step.\"\"\"\n",
    "        if len(targets)!=self.no:\n",
    "            raise ValueError(\"Wrong number of target values.\")\n",
    "        val = (targets-self.zmin)/(self.zmax-self.zmin)\n",
    "        output_deltas = (1-self.ao**2)*(val-self.ao)\n",
    "        hidden_deltas = (1-self.ah**2)*np.dot(self.wo,output_deltas)\n",
    "        self.wo += eps*np.outer(self.ah,output_deltas)+ups*self.co\n",
    "        self.co  = np.outer(self.ah,output_deltas)\n",
    "        self.wi += eps*np.outer(self.ai,hidden_deltas)+ups*self.ci\n",
    "        self.ci  = np.outer(self.ai,hidden_deltas)\n",
    "        error    = 0.5*np.sum( (val-self.ao)**2 )\n",
    "        return(error)\n",
    "\n",
    "    def train(self,tset,iter=1000,eps=0.5,ups=0.1):\n",
    "        \"\"\"Train the network using backward propagation.\n",
    "        The training set is a list, each element is itself a list\n",
    "        the first element of which is the input vector and the second\n",
    "        is the output vector.\n",
    "        The first step is to work out the ranges to scale everything\n",
    "        to lie in [0,1).  The scaling is handled elsewhere.\n",
    "        eps is the \"learning rate\" and ups the \"momentum factor\".\"\"\"\n",
    "        print(\"Training neural network ...\")\n",
    "        self.xmin = np.zeros(len(tset[0][0])) + 1e30\n",
    "        self.xmax = np.zeros(len(tset[0][0])) - 1e30\n",
    "        self.zmin = np.zeros(len(tset[0][1])) + 1e30\n",
    "        self.zmax = np.zeros(len(tset[0][1])) - 1e30\n",
    "        for t in tset:\n",
    "            inp,val = t[0],t[1]\n",
    "            for j in range(len(inp)):\n",
    "                if self.xmin[j]>inp[j]:\n",
    "                    self.xmin[j]=inp[j]\n",
    "                if self.xmax[j]<inp[j]:\n",
    "                    self.xmax[j]=inp[j]\n",
    "            for k in range(len(val)):\n",
    "                if self.zmin[k]>val[k]:\n",
    "                    self.zmin[k]=val[k]\n",
    "                if self.zmax[k]<val[k]:\n",
    "                    self.zmax[k]=val[k]\n",
    "        print(\"Scalings: \",self.xmin,self.xmax,self.zmin,self.zmax)\n",
    "        for i in range(iter):\n",
    "            error=0.0\n",
    "            for t in tset:\n",
    "                self.interp(t[0])\n",
    "                error += self.back_propagate(t[1],eps,ups)\n",
    "            if i%100==0:\n",
    "                print(\"Iteration {:6d} error {:.5f}\".format(i,error))\n",
    "\n",
    "    def test(self,tset,verbose=True):\n",
    "        \"\"\"Test the network on a test set \"tset\".\"\"\"\n",
    "        print(\"Testing neural network...\")\n",
    "        maxerr = 0.0\n",
    "        for t in tset:\n",
    "            if verbose:\n",
    "                print(t[0],\"-> {:10.6f} c.f. {:10.6f}\".format(self.interp(t[0])[0],t[1][0]))\n",
    "            err = np.max( np.abs(self.interp(t[0])-t[1]) )\n",
    "            if err>maxerr:\n",
    "                maxerr = err\n",
    "                maxpnt = t[0]\n",
    "        print(\"Maxerr=\",maxerr,\", at \",maxpnt,\" with value \",self.interp(maxpnt))\n",
    "\n",
    "    def __init__(self,ni,nh,no):\n",
    "        \"\"\"Initialize the class (but do not yet train it).\n",
    "        The arguments are the number of input, hidden and output nodes\n",
    "        (since we have only a single \"hidden\" layer).\n",
    "        \"\"\"\n",
    "        # Set up the number of nodes, include a \"bias\" node in \"ni\".\n",
    "        self.ni,self.nh,self.no = ni+1,nh,no\n",
    "        # Initialize the activations to unity and create random weights.\n",
    "        self.ai= np.zeros(self.ni) + 1.0\n",
    "        self.ah= np.zeros(self.nh) + 1.0\n",
    "        self.ao= np.zeros(self.no) + 1.0\n",
    "        self.wi= np.random.uniform(low=-0.1,high=0.1,size=self.ni*self.nh)\n",
    "        self.wi.shape = (self.ni,self.nh)\n",
    "        self.wo= np.random.uniform(low=-1.0,high=1.0,size=self.nh*self.no)\n",
    "        self.wo.shape = (self.nh,self.no)\n",
    "        # Store some weights for the momenta.\n",
    "        self.ci = np.zeros((self.ni,self.nh),dtype='float')\n",
    "        self.co = np.zeros((self.nh,self.no),dtype='float')\n",
    "        #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example let's consider a network with 2 inputs, a hidden layer of 2 neurons and a single output neuron.  This isn't going to be a particularly powerful interpolator, but we'll see if we can teach it the XOR function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network ...\n",
      "Scalings:  [0. 0.] [1. 1.] [0.] [1.]\n",
      "Iteration      0 error 0.95109\n",
      "Iteration    100 error 0.13764\n",
      "Iteration    200 error 0.00435\n",
      "Iteration    300 error 0.00183\n",
      "Iteration    400 error 0.00114\n",
      "Iteration    500 error 0.00082\n",
      "Iteration    600 error 0.00064\n",
      "Iteration    700 error 0.00170\n",
      "Iteration    800 error 0.00046\n",
      "Iteration    900 error 0.00039\n",
      "Testing neural network...\n",
      "[0, 0] ->   0.001810 c.f.   0.000000\n",
      "[0, 1] ->   0.981729 c.f.   1.000000\n",
      "[1, 0] ->   0.981585 c.f.   1.000000\n",
      "[1, 1] ->  -0.002021 c.f.   0.000000\n",
      "Maxerr= 0.018415163367067278 , at  [1, 0]  with value  [0.98158484]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2,2,1) # 2 inputs, 2 hidden neurons and 1 output neuron.\n",
    "# The training set is the XOR function on two inputs:\n",
    "tset = [ [[0,0],[0]] , [[0,1],[1]], [[1,0],[1]], [[1,1],[0]] ]\n",
    "nn.train(tset)\n",
    "# Now ideally we'd test this network on a different set of data than we used\n",
    "# to train it, but for the XOR function this is kind of difficult since we've\n",
    "# used all of the values in the training ... so we'll just see how well it does:\n",
    "nn.test(tset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights are initialized randomly, and we take only a finite number of steps in our \"training\" to find the minimum of the loss function, we will get subtly different behavior if we just run it again ... the differences will give us a very rough indication of how well converged the network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network ...\n",
      "Scalings:  [0. 0.] [1. 1.] [0.] [1.]\n",
      "Iteration      0 error 1.03353\n",
      "Iteration    100 error 0.13263\n",
      "Iteration    200 error 0.00439\n",
      "Iteration    300 error 0.00184\n",
      "Iteration    400 error 0.00114\n",
      "Iteration    500 error 0.00083\n",
      "Iteration    600 error 0.00064\n",
      "Iteration    700 error 0.00053\n",
      "Iteration    800 error 0.00049\n",
      "Iteration    900 error 0.00039\n",
      "Testing neural network...\n",
      "[0, 0] ->   0.001912 c.f.   0.000000\n",
      "[0, 1] ->   0.981705 c.f.   1.000000\n",
      "[1, 0] ->   0.981563 c.f.   1.000000\n",
      "[1, 1] ->  -0.002028 c.f.   0.000000\n",
      "Maxerr= 0.01843725889012493 , at  [1, 0]  with value  [0.98156274]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2,2,1)\n",
    "nn.train(tset)\n",
    "nn.test(tset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to do a more interesting function.  Let's choose a Gaussian just because.  The such a fascinating function we're going to need many more hidden layer neurons.  In fact what we would probably do is have more than a single hidden layer.  But for now let's just increase the number of hidden layer neurons ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network ...\n",
      "Scalings:  [-1. -1.] [1. 1.] [0.36787944] [1.]\n",
      "Iteration      0 error 4.89470\n",
      "Iteration    100 error 6.06672\n",
      "Iteration    200 error 6.10429\n",
      "Iteration    300 error 6.06479\n",
      "Iteration    400 error 5.42991\n",
      "Iteration    500 error 3.32821\n",
      "Iteration    600 error 2.49596\n",
      "Iteration    700 error 2.46944\n",
      "Iteration    800 error 2.47539\n",
      "Iteration    900 error 2.48885\n",
      "Iteration   1000 error 2.50366\n",
      "Iteration   1100 error 2.51699\n",
      "Iteration   1200 error 2.52645\n",
      "Iteration   1300 error 2.52829\n",
      "Iteration   1400 error 2.51489\n",
      "Iteration   1500 error 2.46899\n",
      "Iteration   1600 error 2.34883\n",
      "Iteration   1700 error 2.05446\n",
      "Iteration   1800 error 1.44130\n",
      "Iteration   1900 error 0.79872\n",
      "Iteration   2000 error 0.60264\n",
      "Iteration   2100 error 0.58620\n",
      "Iteration   2200 error 0.57843\n",
      "Iteration   2300 error 0.57066\n",
      "Iteration   2400 error 0.56600\n",
      "Iteration   2500 error 0.56336\n",
      "Iteration   2600 error 0.56153\n",
      "Iteration   2700 error 0.55994\n",
      "Iteration   2800 error 0.55832\n",
      "Iteration   2900 error 0.55656\n",
      "Iteration   3000 error 0.55460\n",
      "Iteration   3100 error 0.55238\n",
      "Iteration   3200 error 0.54986\n",
      "Iteration   3300 error 0.54699\n",
      "Iteration   3400 error 0.54372\n",
      "Iteration   3500 error 0.54001\n",
      "Iteration   3600 error 0.53582\n",
      "Iteration   3700 error 0.53108\n",
      "Iteration   3800 error 0.52578\n",
      "Iteration   3900 error 0.51987\n",
      "Iteration   4000 error 0.51334\n",
      "Iteration   4100 error 0.50619\n",
      "Iteration   4200 error 0.49843\n",
      "Iteration   4300 error 0.49010\n",
      "Iteration   4400 error 0.48126\n",
      "Iteration   4500 error 0.47197\n",
      "Iteration   4600 error 0.46231\n",
      "Iteration   4700 error 0.45238\n",
      "Iteration   4800 error 0.44226\n",
      "Iteration   4900 error 0.43202\n",
      "Iteration   5000 error 0.42175\n",
      "Iteration   5100 error 0.41148\n",
      "Iteration   5200 error 0.40127\n",
      "Iteration   5300 error 0.39115\n",
      "Iteration   5400 error 0.38115\n",
      "Iteration   5500 error 0.37128\n",
      "Iteration   5600 error 0.36156\n",
      "Iteration   5700 error 0.35199\n",
      "Iteration   5800 error 0.34260\n",
      "Iteration   5900 error 0.33340\n",
      "Iteration   6000 error 0.32442\n",
      "Iteration   6100 error 0.31569\n",
      "Iteration   6200 error 0.30724\n",
      "Iteration   6300 error 0.29911\n",
      "Iteration   6400 error 0.29135\n",
      "Iteration   6500 error 0.28399\n",
      "Iteration   6600 error 0.27708\n",
      "Iteration   6700 error 0.27064\n",
      "Iteration   6800 error 0.26472\n",
      "Iteration   6900 error 0.25932\n",
      "Iteration   7000 error 0.25444\n",
      "Iteration   7100 error 0.25009\n",
      "Iteration   7200 error 0.24625\n",
      "Iteration   7300 error 0.24288\n",
      "Iteration   7400 error 0.23995\n",
      "Iteration   7500 error 0.23743\n",
      "Iteration   7600 error 0.23527\n",
      "Iteration   7700 error 0.23342\n",
      "Iteration   7800 error 0.23185\n",
      "Iteration   7900 error 0.23051\n"
     ]
    }
   ],
   "source": [
    "tset = []\n",
    "for x1 in np.linspace(-1,1,15):\n",
    "    for x2 in np.linspace(-1,1,15):\n",
    "        ff = np.exp( -0.5*(x1**2+x2**2) )\n",
    "        tset.append( [ [x1,x2], [ff] ] )\n",
    "#\n",
    "nn = NeuralNetwork(2,50,1)\n",
    "# There are automated ways of setting \"eps\" and \"ups\", but for\n",
    "# now let's just set them to small numbers and up the number\n",
    "# of iterations ...\n",
    "nn.train(tset,iter=8000,eps=0.003,ups=0.003)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neural network...\n",
      "[-0.9251164712385394, -0.2653328025302264] ->   0.610451 c.f.   0.629316\n",
      "[-0.9251164712385394, 0.2695312278399009] ->   0.644263 c.f.   0.628610\n",
      "[-0.9251164712385394, 0.851178146918123] ->   0.460166 c.f.   0.453767\n",
      "[-0.10613332212495363, -0.5029047831595017] ->   0.870571 c.f.   0.876263\n",
      "[-0.10613332212495363, -0.057667308801595984] ->   0.916156 c.f.   0.992732\n",
      "[-0.10613332212495363, 0.007333263214478691] ->   0.915140 c.f.   0.994357\n",
      "[0.3730609074939666, -0.8362086775982078] ->   0.667955 c.f.   0.657566\n",
      "[0.3730609074939666, 0.24493947429179808] ->   0.875094 c.f.   0.905213\n",
      "[0.3730609074939666, 0.9241131217954823] ->   0.607272 c.f.   0.608609\n",
      "Maxerr= 0.07921709648093689 , at  [-0.10613332212495363, 0.007333263214478691]  with value  [0.91513986]\n"
     ]
    }
   ],
   "source": [
    "# Let's pick a few random values to test:\n",
    "tset = []\n",
    "for x1 in sorted(np.random.uniform(low=-1.,high=1.0,size=3)):\n",
    "    for x2 in sorted(np.random.uniform(low=-1.,high=1.0,size=3)):\n",
    "        ff = np.exp( -0.5*(x1**2+x2**2) )\n",
    "        tset.append( [ [x1,x2], [ff] ] )\n",
    "#\n",
    "nn.test(tset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it doesn't do too badly, though it's not exactly high precision as we've coded it up.  Given more training data and a bigger network we could make it perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
